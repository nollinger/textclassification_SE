'''
@author: noll_richard
'''



'''

1. creating functions for preprocessing/word embedding of data.

'''

from string import punctuation
from string import digits

def remove_umlaut(text): 
    
    """
    Replace umlauts for a given text
    
    :param text: text as string
    :return: manipulated text as str
    """
    
    tempVar = text # local variable
    
    # Using str.replace() 
    
    tempVar = tempVar.replace('ä', 'ae')
    tempVar = tempVar.replace('ö', 'oe')
    tempVar = tempVar.replace('ü', 'ue')
    tempVar = tempVar.replace('Ä', 'Ae')
    tempVar = tempVar.replace('Ö', 'Oe')
    tempVar = tempVar.replace('Ü', 'Ue')
    tempVar = tempVar.replace('ß', 'ss')
    tempVar = tempVar.replace('ãÿ', 'ss')
    tempVar = tempVar.replace('ÃŸ', 'ss')
    tempVar = tempVar.replace('Ã¶', 'oe')
    tempVar = tempVar.replace('Ã¼', 'ue')
    tempVar = tempVar.replace('ã¼', 'ue')
    tempVar = tempVar.replace('Ã¤', 'ae')
    
    return tempVar



def punctuation_number(text):
    
    """
    Delete punctuation '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
    and digits '0-9'
    
    :return: text without punctuation and digits
    """
    
    remove_pun = str.maketrans('', '', punctuation)
    text_wo_pun = text.translate(remove_pun)
    remove_digits = str.maketrans('', '', digits) #using native string function
    text_wo_num_pun = text_wo_pun.translate(remove_digits)
    return text_wo_num_pun


# import stop words and tokenization
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

text_file = open("C:\\Users\\nollr\\Desktop\\stop_words_german.txt", "r")
stopword = text_file.read().split()
text_file.close()

# tokenize data: (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams
vects_NB = CountVectorizer(stop_words=stopword, ngram_range=(1, 1), max_df=0.5, min_df=0.0) 
vects_SVM = CountVectorizer(stop_words=stopword, ngram_range=(1, 1), min_df=0.01, max_df=0.5) 
vects_KNN = CountVectorizer(stop_words=stopword, ngram_range=(1, 1), min_df=0.01, max_df=0.8) 

tf_idf_NB = TfidfTransformer(use_idf = True) #tf idf 
tf_idf_SVM = TfidfTransformer(use_idf = False)
tf_idf_KNN = TfidfTransformer(use_idf = True)
'''

2. import of the training data

'''


import pandas as pd
import os
import numpy as np


training_lst=[] #list with all the training data
disease=[] #list with disease names

for root, dirs, files in os.walk("C:\\Users\\nollr\\Desktop\\SD_DATEN\\SEMD"): 
   #path with the training-data for both diseases.
   for name in files:
      if name.endswith(".txt"): # data is saved as a txt file.
          file = os.path.join(root, name)
          file= open(file, "r")
          lines= file.read()
          lines = remove_umlaut(lines) #replacing the 'umlaute'
          lines = punctuation_number(lines) #removing punctuation and numbers
          training_lst.append(lines)
          if name.startswith('CF+'):
              disease.append('CF+')
          elif name.startswith('SD+'):
              disease.append('SD+')
          file.close()
          
#print (training_lst[0])
#print (disease[0:10])


'''

3. transform data into csv file and create data matrix.

'''

dict = {
        'text': training_lst,
        'SE': disease
        }

df = pd.DataFrame(dict)
df.to_csv('SE1.csv',index=False,header=True) # store data in csv with two columns.
#df

data = pd.read_csv('SE1.csv') #text in column 1, classifier in column 2.
numpy_array = data.as_matrix() #create data matrix
X = numpy_array[:,0] #text
Y = numpy_array[:,1] #classifier

#train_test_split
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(
X, Y, train_size = 200, random_state=7)

'''
i = 0
for x in Y_train:
    if x == 'SD+':
        i +=1

i       
'''    

'''

4. introducing and creating a pipeline for different classifiers

'''



from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB #import of classifiers
from sklearn.linear_model import SGDClassifier #sgd: stochastic gradient descent learning
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier #multy layer perceptron (backpropagation)

def classifier(classifier):
    
    '''
    
    different classifier
    
    '''
    
    if classifier == 'NB':
        classifier = MultinomialNB(alpha=0.01)
    elif classifier == 'SVM':
        classifier = SGDClassifier(loss='hinge', penalty='l2',
                                            alpha=0.001, random_state=42)
    elif classifier == 'KNN':
        classifier = KNeighborsClassifier()
   
    elif classifier == 'MLP':
        classifier = MLPClassifier()
    
    return classifier
    

text_clf_SVM = Pipeline([('vect', vects_SVM),
 ('tfidf', tf_idf_SVM),
 ('clf', classifier('SVM')) # choose classifier
])

text_clf_NB = Pipeline([('vect', vects_NB),
 ('tfidf', tf_idf_NB),
 ('clf', classifier('NB')) # choose classifier
])
  
text_clf_KNN = Pipeline([('vect', vects_KNN),
 ('tfidf', tf_idf_KNN),
 ('clf', classifier('KNN')) # choose classifier
])
    
text_clf_MLP = Pipeline([('vect', vects),
 ('tfidf', tf_idf_KNN),
 ('clf', classifier('MLP')) # choose classifier
])

  
    
'''

5. GridSearch cross validation (finding best parameters)


'''


from sklearn.model_selection import GridSearchCV
parameters = {'vect__ngram_range': [(1, 1), (1, 2)],
                'vect__max_df': (0.2, 0.5, 0.8, 1.0),
                'vect__min_df': (0.0, 0.01, 0.02, 1),
              'tfidf__use_idf': (True, False),
              #'clf__alpha': (1e-2, 1e-3)
    
              }

gs_clf = GridSearchCV(text_clf_KNN, parameters, n_jobs=-1, scoring='f1_macro')
gs_clf = gs_clf.fit(X_train, Y_train)

gs_clf.best_score_
gs_clf.best_params_ 
    

text_clf_SVM.get_params().keys()
    
    
    
'''

6. Using cross validation

'''


from sklearn.model_selection import cross_validate
from sklearn.metrics import recall_score
from sklearn import metrics
from sklearn.model_selection import ShuffleSplit

scoring = ['precision_macro', 'recall_macro', 'f1_macro']
cv_SVM = ShuffleSplit(n_splits= 10, test_size=0.2, random_state=0)
cv_NB = ShuffleSplit(n_splits= 100, test_size=0.2, random_state=0) #validation splits and test size
cv_KNN = ShuffleSplit(n_splits= 100, test_size=0.2, random_state=0)
cv_MLP = ShuffleSplit(n_splits= 3, test_size=0.2, random_state=0)
scores_SVM = cross_validate(text_clf_SVM, X_train, Y_train, cv=cv_SVM, scoring=scoring, return_estimator=False)
scores_NB = cross_validate(text_clf_NB, X_train, Y_train, cv=cv_NB, scoring=scoring, return_estimator=False)
scores_KNN = cross_validate(text_clf_KNN, X_train, Y_train, cv=cv_KNN, scoring=scoring, return_estimator=False)
scores_MLP = cross_validate(text_clf_MLP, X_train, Y_train, cv=cv_MLP, scoring=scoring, return_estimator=False)

#sorted(scores.keys())

print('\n SVM measures:')
print('f1-score: ', scores_SVM['test_f1_macro'].mean())
print('precision: ', scores_SVM['test_precision_macro'].mean())
print('recall: ', scores_SVM['test_recall_macro'].mean())


print('\n NB measures:')
print('f1-score: ', scores_NB['test_f1_macro'].mean())
print('precision: ', scores_NB['test_precision_macro'].mean())
print('recall: ', scores_NB['test_recall_macro'].mean())

print('\n KNN measures:')
print('f1-score: ', scores_KNN['test_f1_macro'].mean())
print('precision: ', scores_KNN['test_precision_macro'].mean())
print('recall: ', scores_KNN['test_recall_macro'].mean())

print('\n MLP measures:')
print('f1-score: ', scores_MLP['test_f1_macro'].mean())
print('precision: ', scores_MLP['test_precision_macro'].mean())
print('recall: ', scores_MLP['test_recall_macro'].mean())
#text_clf.get_params()







'''

7. learning curve about the effect of different training sizes on validation scores.


'''




import matplotlib.pyplot as plt 
from sklearn.model_selection import learning_curve


def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,
                        n_jobs=None, train_size=np.linspace(.1, 1.0, 5)):
    


    axes.set_title(title)
    if ylim is not None:
        axes.set_ylim(*ylim)
    axes.set_xlabel("Training examples")
    axes.set_ylabel("Score")

    train_sizes, train_scores, test_scores = \
        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,
                       train_sizes=train_size, scoring ='f1_macro'
                       )
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    
    axes.grid()
    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,
                         train_scores_mean + train_scores_std, alpha=0.1,
                         color="r")
    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,
                         test_scores_mean + test_scores_std, alpha=0.1,
                         color="g")
    axes.plot(train_sizes, train_scores_mean, 'o-', color="r",
                 label="Training score")
    axes.plot(train_sizes, test_scores_mean, 'o-', color="g",
                 label="Cross-validation score")
    axes.legend(loc="best")
    
    
    return plt, train_scores_mean, test_scores_mean


fig,axes = plt.subplots(1, 3, figsize=(10, 15))

title1 = "Learning Curves (Naive Bayes)"

plot_learning_curve(text_clf_NB, title1, X_train, y=Y_train, axes=axes[0], ylim=(0.6, 1.01),
                    cv=cv_NB, n_jobs=4)


title2 = "SVM"

plot_learning_curve(text_clf_SVM, title2, X_train, y=Y_train, axes=axes[1], ylim=(0.6, 1.01),
                    cv=cv_SVM, n_jobs=4)

title3 = "KNN"

plot_learning_curve(text_clf_KNN, title3, X_train, y=Y_train, axes=axes[2], ylim=(0.6, 1.01),
                    cv=cv_KNN, n_jobs=4)

title4 = "MLP"

plot_learning_curve(text_clf_MLP, title4, X_train, y=Y_train, axes=axes[3], ylim=(0.6, 1.01),
                    cv=cv_MLP, n_jobs=4)
plt.show()






'''

8. prediction of new cases

'''


from sklearn.metrics import f1_score, precision_score, recall_score

pred_NB = text_clf_NB.fit(X_train, Y_train).predict(X_test)
pred_SVM = text_clf_SVM.fit(X_train, Y_train).predict(X_test)
pred_KNN = text_clf_KNN.fit(X_train, Y_train).predict(X_test)

print('\nNB_PRECISION: ', precision_score(pred_NB, Y_test, average='macro'))
print('NB_RECALL: ', recall_score(pred_NB, Y_test, average='macro'))
print('NB_F1: ', f1_score(pred_NB, Y_test, average='macro'))

print('\nSVM_PRECISION: ', precision_score(pred_SVM, Y_test, average='macro'))
print('SVM_RECALL: ', recall_score(pred_SVM, Y_test, average='macro'))
print('SVM_F1: ', f1_score(pred_SVM, Y_test, average='macro'))

print('\nKNN_PRECISION: ', precision_score(pred_KNN, Y_test, average='macro'))
print('KNN_RECALL: ', recall_score(pred_KNN, Y_test, average='macro'))
print('KNN_F1: ', f1_score(pred_KNN, Y_test, average='macro'))
